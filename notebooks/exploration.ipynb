{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XAUUSD Condition-Based Signal Engine - Exploration Notebook\n",
        "\n",
        "This notebook demonstrates the complete workflow of the Dr. Chen-style signal generation system:\n",
        "\n",
        "1. **Data Loading**: Load minute OHLCV and quotes data\n",
        "2. **Feature Engineering**: Build the feature matrix\n",
        "3. **Labeling**: Generate labels for 5m, 15m, 30m horizons\n",
        "4. **Model Training**: Train XGBoost classifiers\n",
        "5. **Signal Generation**: Generate signals with SL/TP levels\n",
        "6. **Evaluation**: Analyze model and backtest performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path for imports\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Project imports\n",
        "from src.data_loader import (\n",
        "    load_minute_bars,\n",
        "    load_quotes,\n",
        "    get_combined_dataset,\n",
        "    get_combined_dataset_multi_year\n",
        ")\n",
        "from src.feature_engineering import build_feature_matrix, get_feature_info\n",
        "from src.labeling import generate_labels_for_all_horizons, get_label_statistics\n",
        "from src.model_training import (\n",
        "    train_all_horizon_models,\n",
        "    get_feature_importance_ranking,\n",
        "    time_series_train_test_split\n",
        ")\n",
        "from src.signal_generator import (\n",
        "    generate_signals_for_latest_row,\n",
        "    format_signals_summary\n",
        ")\n",
        "from src.backtest import run_backtest_all_horizons\n",
        "from src.evaluation import evaluate_backtest_results, compare_horizons\n",
        "from src.utils.plotting_utils import (\n",
        "    set_plotting_style,\n",
        "    plot_feature_distributions,\n",
        "    plot_label_distribution,\n",
        "    plot_confusion_matrix,\n",
        "    plot_feature_importance,\n",
        "    plot_equity_curve\n",
        ")\n",
        "from src.config import (\n",
        "    HORIZONS,\n",
        "    VOL_PARAMS,\n",
        "    FEATURE_COLUMNS,\n",
        "    MODEL_DIR,\n",
        "    MINUTE_OHLCV_DIR,\n",
        "    QUOTES_DIR\n",
        ")\n",
        "\n",
        "# Set plotting style\n",
        "set_plotting_style()\n",
        "\n",
        "print(\"Imports complete!\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load minute OHLCV data and quotes, then align them to create a combined dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - adjust paths and years as needed\n",
        "DATA_DIR = project_root.parent / \"Data\"\n",
        "MINUTE_DIR = DATA_DIR / \"ohlcv_minute\"\n",
        "QUOTES_DIR_PATH = DATA_DIR / \"quotes\"\n",
        "\n",
        "# Choose years to load (start with 1-2 years for faster exploration)\n",
        "YEARS_TO_LOAD = [2023, 2024]  # Adjust as needed\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Loading years: {YEARS_TO_LOAD}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load combined dataset\n",
        "print(\"Loading and combining data...\")\n",
        "\n",
        "try:\n",
        "    df = get_combined_dataset_multi_year(\n",
        "        minute_dir=str(MINUTE_DIR),\n",
        "        quotes_dir=str(QUOTES_DIR_PATH),\n",
        "        years=YEARS_TO_LOAD,\n",
        "        symbol=\"XAUUSD\"\n",
        "    )\n",
        "    print(f\"\\nLoaded {len(df):,} rows\")\n",
        "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nTrying single year load...\")\n",
        "    # Fallback to single file if multi-year fails\n",
        "    df = get_combined_dataset(\n",
        "        minute_path=str(MINUTE_DIR / \"XAUUSD_minute_2024.parquet\"),\n",
        "        quotes_path=str(QUOTES_DIR_PATH / \"XAUUSD_quotes_2024.parquet\")\n",
        "    )\n",
        "    print(f\"Loaded {len(df):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect raw data\n",
        "print(\"Raw data columns:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "Build the complete feature matrix including:\n",
        "- Microstructure features (mid, spread, imbalance)\n",
        "- Volatility features (log returns, sigma)\n",
        "- VWAP features\n",
        "- Time features\n",
        "- News placeholders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display available features\n",
        "feature_info = get_feature_info()\n",
        "print(\"Available feature categories:\")\n",
        "for category, features in feature_info.items():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    for name, desc in features.items():\n",
        "        print(f\"  - {name}: {desc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build feature matrix\n",
        "print(\"Building feature matrix...\")\n",
        "df = build_feature_matrix(df, vol_lookback=60, vwap_lookback=60)\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {df.shape}\")\n",
        "print(f\"\\nAll columns: {df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature distributions\n",
        "features_to_plot = ['mid', 'spread_pct', 'imbalance', 'log_ret', 'sigma', \n",
        "                    'vwap_deviation', 'minute_of_day', 'day_of_week']\n",
        "fig = plot_feature_distributions(df, features_to_plot, ncols=4, figsize=(16, 8))\n",
        "plt.suptitle(\"Feature Distributions\", y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Labeling\n",
        "\n",
        "Generate environment labels for 5m, 15m, and 30m horizons using volatility-based SL/TP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display horizon parameters\n",
        "print(\"Horizon Parameters:\")\n",
        "for horizon, params in HORIZONS.items():\n",
        "    print(f\"  {horizon}: minutes={params['minutes']}, k1={params['k1']}, k2={params['k2']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate labels\n",
        "print(\"Generating labels for all horizons...\")\n",
        "df = generate_labels_for_all_horizons(df, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get label statistics\n",
        "label_stats = get_label_statistics(df)\n",
        "print(\"\\nLabel Statistics:\")\n",
        "for label_col, stats in label_stats.items():\n",
        "    print(f\"\\n{label_col}:\")\n",
        "    print(f\"  Total labeled: {stats['count']:,}\")\n",
        "    print(f\"  NaN count: {stats['nan_count']:,}\")\n",
        "    print(f\"  Class distribution:\")\n",
        "    for cls, pct in stats['class_pcts'].items():\n",
        "        cls_name = {-1: 'Short', 0: 'Flat', 1: 'Long'}.get(cls, str(cls))\n",
        "        print(f\"    {cls_name}: {pct:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot label distributions\n",
        "fig = plot_label_distribution(df)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training\n",
        "\n",
        "Train XGBoost classifiers for each horizon with time-based train/test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory\n",
        "models_dir = project_root / \"models\"\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "print(f\"Models will be saved to: {models_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all horizon models\n",
        "print(\"Training models for all horizons...\\n\")\n",
        "training_results = train_all_horizon_models(\n",
        "    df=df,\n",
        "    model_dir=str(models_dir),\n",
        "    train_ratio=0.8,\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance rankings\n",
        "importance_rankings = get_feature_importance_ranking(training_results, top_n=15)\n",
        "\n",
        "print(\"Top 15 Most Important Features by Horizon:\\n\")\n",
        "for horizon, ranking in importance_rankings.items():\n",
        "    print(f\"\\n{horizon}:\")\n",
        "    for i, (feat, imp) in enumerate(ranking[:10], 1):\n",
        "        print(f\"  {i}. {feat}: {imp:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature importances\n",
        "for horizon in [\"5m\", \"15m\", \"30m\"]:\n",
        "    if horizon in training_results:\n",
        "        fig = plot_feature_importance(\n",
        "            training_results[horizon][\"feature_importance\"],\n",
        "            top_n=15,\n",
        "            title=f\"Feature Importance - {horizon} Horizon\"\n",
        "        )\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "for horizon in [\"5m\", \"15m\", \"30m\"]:\n",
        "    if horizon in training_results:\n",
        "        fig = plot_confusion_matrix(\n",
        "            training_results[horizon][\"confusion_matrix\"],\n",
        "            title=f\"Confusion Matrix - {horizon} Horizon\"\n",
        "        )\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Signal Generation\n",
        "\n",
        "Generate signals using the trained models on recent data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate signals for the latest row\n",
        "print(\"Generating signals for the latest data point...\\n\")\n",
        "\n",
        "signals = generate_signals_for_latest_row(\n",
        "    df=df,\n",
        "    model_dir=str(models_dir),\n",
        "    vol_params=VOL_PARAMS,\n",
        "    threshold=0.6\n",
        ")\n",
        "\n",
        "# Print formatted summary\n",
        "print(format_signals_summary(signals))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed signal inspection\n",
        "print(\"Detailed Signal Information:\\n\")\n",
        "for horizon, sig_data in signals[\"signals\"].items():\n",
        "    print(f\"{horizon} Horizon:\")\n",
        "    print(f\"  Signal: {sig_data['signal']}\")\n",
        "    print(f\"  Confidence: {sig_data['confidence']:.2%}\")\n",
        "    print(f\"  Probabilities: Short={sig_data['probabilities'][0]:.2%}, \"\n",
        "          f\"Flat={sig_data['probabilities'][1]:.2%}, \"\n",
        "          f\"Long={sig_data['probabilities'][2]:.2%}\")\n",
        "    if sig_data['sl_price']:\n",
        "        print(f\"  SL Price: ${sig_data['sl_price']:.2f}\")\n",
        "        print(f\"  TP Price: ${sig_data['tp_price']:.2f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Backtesting\n",
        "\n",
        "Run a simple backtest on out-of-sample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for backtesting (use last 20% as test)\n",
        "train_df, test_df = time_series_train_test_split(df, train_ratio=0.8)\n",
        "\n",
        "print(f\"Backtest period: {test_df.index.min()} to {test_df.index.max()}\")\n",
        "print(f\"Backtest rows: {len(test_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run backtest (may take a few minutes)\n",
        "print(\"Running backtest...\")\n",
        "\n",
        "backtest_results = run_backtest_all_horizons(\n",
        "    df=test_df,\n",
        "    model_dir=str(models_dir),\n",
        "    confidence_threshold=0.6,\n",
        "    max_trades_per_horizon=500,  # Limit for faster testing\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate backtest results\n",
        "summaries = evaluate_backtest_results(backtest_results, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare horizons\n",
        "comparison_df = compare_horizons(summaries)\n",
        "print(\"\\nHorizon Comparison:\")\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot equity curves\n",
        "for horizon in [\"5m\", \"15m\", \"30m\"]:\n",
        "    if horizon in backtest_results and len(backtest_results[horizon]) > 0:\n",
        "        fig = plot_equity_curve(\n",
        "            backtest_results[horizon],\n",
        "            title=f\"Equity Curve - {horizon} Horizon\"\n",
        "        )\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analysis & Insights\n",
        "\n",
        "Additional analysis of the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze trades by direction\n",
        "for horizon in [\"5m\", \"15m\", \"30m\"]:\n",
        "    if horizon not in backtest_results or len(backtest_results[horizon]) == 0:\n",
        "        continue\n",
        "    \n",
        "    trades = backtest_results[horizon]\n",
        "    print(f\"\\n{horizon} Trades by Direction:\")\n",
        "    \n",
        "    for direction, name in [(1, \"Long\"), (-1, \"Short\")]:\n",
        "        subset = trades[trades[\"direction\"] == direction]\n",
        "        if len(subset) > 0:\n",
        "            win_rate = (subset[\"pnl_ret\"] > 0).mean()\n",
        "            avg_ret = subset[\"pnl_ret\"].mean()\n",
        "            print(f\"  {name}: {len(subset)} trades, Win Rate: {win_rate:.1%}, Avg Return: {avg_ret:.4%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze trades by exit reason\n",
        "for horizon in [\"5m\", \"15m\", \"30m\"]:\n",
        "    if horizon not in backtest_results or len(backtest_results[horizon]) == 0:\n",
        "        continue\n",
        "    \n",
        "    trades = backtest_results[horizon]\n",
        "    print(f\"\\n{horizon} Trades by Exit Reason:\")\n",
        "    \n",
        "    for reason in trades[\"exit_reason\"].unique():\n",
        "        subset = trades[trades[\"exit_reason\"] == reason]\n",
        "        if len(subset) > 0:\n",
        "            win_rate = (subset[\"pnl_ret\"] > 0).mean()\n",
        "            avg_ret = subset[\"pnl_ret\"].mean()\n",
        "            print(f\"  {reason}: {len(subset)} trades, Win Rate: {win_rate:.1%}, Avg Return: {avg_ret:.4%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete workflow of the XAUUSD Condition-Based Signal Engine:\n",
        "\n",
        "1. **Data Loading**: Successfully loaded and aligned minute OHLCV with quotes data\n",
        "2. **Feature Engineering**: Built comprehensive features including microstructure, volatility, VWAP, and time\n",
        "3. **Labeling**: Generated environment labels using volatility-based SL/TP thresholds\n",
        "4. **Model Training**: Trained XGBoost classifiers with reasonable accuracy\n",
        "5. **Signal Generation**: Demonstrated real-time signal output with SL/TP levels\n",
        "6. **Backtesting**: Evaluated out-of-sample performance\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Tune confidence thresholds for better precision vs. recall trade-off\n",
        "- Experiment with different k1/k2 multipliers\n",
        "- Add actual news calendar integration\n",
        "- Implement more sophisticated position sizing\n",
        "- Consider walk-forward validation for more robust estimates\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
